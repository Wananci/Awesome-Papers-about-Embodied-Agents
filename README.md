# Awesome Papers about VLM for Embodiment Task

A curated list of influential papers about VLM for embodiment task which is maintained by [Wananci](https://github.com/Wananci). 

## Table of Contents
- [Introduction](#introduction)
- [Paper Categories](#paper-categories)
  - [Primitive Skill](#primitive-skill)
  - [Learning in Embodied Agents](#learning-in-embodied-agents)
  - [Simulation Environments](#simulation-environments)
  - [Applications](#applications)
- [Contributing](#contributing)
- [License](#license)

# Introduction

Embodied agents are AI-driven systems designed to interact with the physical or virtual world through a body or avatar, enabling perception, action, and communication. These agents combine sensory inputs (e.g., vision, speech) with cognitive processes to perform tasks, simulate human-like behaviors, or engage in immersive interactions. They are widely used in robotics, virtual assistants, and gaming for more natural and context-aware interactions.

## üöÄ Primitive Skill
- [**Primitive Skill-based Robot Learning from Human Evaluative Feedback**](http://arxiv.org/abs/2307.15801) [**arXiv 2023.07**] [[**üîó Code**]](https://seediros23.github.io/)  
  <details>
    <summary>üîç <b>Highlight</b></summary>

    - **Human Evaluation**: Leveraging human feedback to guide learning.  
    - **Primitive Skill**: Focused on distinct robot manipulation skills.  
    - **Parameter Policy**: Adjust parameters for different tasks.  

    ![SEED Architecture](./imgs/SEED.png)
  </details>  
  **Authors**: Ayano Hiranaka*<sup>1</sup>, Minjune Hwang*<sup>2</sup>, Sharon Lee<sup>2</sup>, Chen Wang<sup>2</sup>, Li Fei-Fei<sup>2</sup>, Jiajun Wu<sup>2</sup>, Ruohan Zhang<sup>2</sup>  
  <sup>1</sup>Department of Mechanical Engineering, Stanford University  
  <sup>2</sup>Department of Computer Science, Stanford University  



  